# -*- coding: utf-8 -*-
"""turingtech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9JP1fuvIzc3pYyvcpEwz1Raif2Scs0I
"""

pip install pandas numpy scikit-learn nltk spacy gensim

#importing the needed libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import spacy
from gensim import corpora, models
import gensim

#importing the data set
data = pd.read_csv('/news.csv')

# Checking the shape of the data
print(data.shape)

# Displaying  the first few rows of the dataset
print(data.head())

# Extracting  the labels
labels = data.label
print(labels.head())

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(data['text'], labels, test_size=0.2, random_state=7)

# Sentiment Analysis
nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()
data['sentiment'] = data['text'].apply(lambda x: sid.polarity_scores(x)['compound'])

# Named Entity Recognition (NER)
nlp = spacy.load('en_core_web_sm')
data['entities'] = data['text'].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])

# Topic Modeling
def preprocess(text):
    return [word for word in gensim.utils.simple_preprocess(text) if word not in gensim.parsing.preprocessing.STOPWORDS]

texts = data['text'].apply(preprocess)
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]
lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)
data['topics'] = [lda_model[doc] for doc in corpus]

# Initialize TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)

# Fit and transform the training set, and transform the testing set
tfidf_train = tfidf_vectorizer.fit_transform(x_train)
tfidf_test = tfidf_vectorizer.transform(x_test)

# Initialize PassiveAggressiveClassifier
pac = PassiveAggressiveClassifier(max_iter=50)
pac.fit(tfidf_train, y_train)

# Initialize PassiveAggressiveClassifier
pac = PassiveAggressiveClassifier(max_iter=50)
pac.fit(tfidf_train, y_train)

# Predict on the test set and calculate accuracy
y_pred = pac.predict(tfidf_test)
score = accuracy_score(y_test, y_pred)
print(f'Accuracy: {round(score*100,2)}%')

# Build confusion matrix
confusion = confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL'])
print(confusion)

# Display some of the additional features
print(data[['text', 'sentiment', 'entities', 'topics']].head())

